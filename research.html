<html lang="en-US"><head>
    
<style>
 
.center {
 text-align: center
}
 
</style>
    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Japanese Linguist Software Engineer | Matthew Bieda</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Japanese Linguist Software Engineer">
<meta property="og:locale" content="en_US">
<meta name="description" content="Created for my CS50x final project.">
<meta property="og:description" content="Created for my CS50x final project.">
<link rel="canonical" href="https://matthewbieda.github.io/">
<meta property="og:url" content="https://matthewbieda.github.io/">
<meta property="og:site_name" content="Matthew Bieda">
<script type="application/ld+json">
{"@type":"WebSite","headline":"Japanese Linguist Software Engineer","url":"https://matthewbieda.github.io/","description":"Created for my CS50x final project.","name":"Matthew Bieda","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/assets/css/style.css?v=3480b759b1c98989742c6797e5d49c733b167806">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machine Translation</h1>
      <h2 class="project-tagline">A historical overview.</h2>
      
      
    </section>

    <section class="main-content">
        <h2 class="center">Rule-Based Machine Translation (RBMT)</h2>
        
        Rule-based machine translation (RBMT; "Classical Approach" of MT) is machine translation systems based on linguistic information about source and target languages basically retrieved from (unilingual, bilingual or multilingual) dictionaries and grammars covering the main semantic, morphological, and syntactic regularities of each language respectively. Having input sentences (in some source language), an RBMT system generates them to output sentences (in some target language) on the basis of morphological, syntactic, and semantic analysis of both the source and the target languages involved in a concrete translation task.

        <h2 class="center">Statistical Machine Translation (SMT)</h2>
        
        Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora. The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.
        The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Before the introduction of neural machine translation, it was by far the most widely studied machine translation method.

        <h2 class="center">Neural Machine Translation (NMT)</h2>
        
        Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

        NMT departs from phrase-based statistical approaches that use separately engineered subcomponents. Neural machine translation (NMT) is not a drastic step beyond what has been traditionally done in statistical machine translation (SMT). Its main departure is the use of vector representations ("embeddings", "continuous space representations") for words and internal states. The structure of the models is simpler than phrase-based models. There is no separate language model, translation model, and reordering model, but just a single sequence model that predicts one word at a time. However, this sequence prediction is conditioned on the entire source sentence and the entire already produced target sequence. NMT models use deep learning and representation learning.

        <h2 class="center">Open NMT and my research</h2>

        OpenNMT is an open source ecosystem for neural machine translation and neural sequence learning. My project aims to build a bilingual Japanese --> English translation system leveraging OpenNMT and my linguistic knowledge and compare it to industry leading MNMT (multilingual NMT) software such as Google Translate and DeepL uses the RIBES evaluation metric. This is to evaluate a proprietary vs. open source implementation and to study the effects of a bilingual vs. multilingual model on a high-resource language such as Japanese. 


        
<p><a href="index.html">Homepage</a></p>

    </section>



</body></html>

